{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: dumbbells.txt\n",
      "(151, 75)\n",
      "Processing: face_wipe.txt\n",
      "(163, 75)\n",
      "Processing: left.txt\n",
      "(199, 75)\n",
      "Processing: muscle.txt\n",
      "(159, 75)\n",
      "Processing: nothing.txt\n",
      "(225, 75)\n",
      "Processing: pac_man.txt\n",
      "(79, 75)\n",
      "Processing: right.txt\n",
      "(225, 75)\n",
      "Processing: shooting_star.txt\n",
      "(73, 75)\n",
      "Processing: shout_out.txt\n",
      "(23, 75)\n",
      "Processing: tornado.txt\n",
      "(9, 75)\n",
      "Processing: weight_lifting.txt\n",
      "(65, 75)\n"
     ]
    }
   ],
   "source": [
    "#import and construct hepler function\n",
    "from util_main import load_dance_dance_data_set\n",
    "from util_main import test_model_nn\n",
    "from util_main import load_model as load_model_keras\n",
    "from util_main import one_hot_encode_labels\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data_set_path = \"Data-sets/Dance_Data\"\n",
    "sampling_rate = 5\n",
    "window_length = 2.56  # Seconds\n",
    "\n",
    "#load data set\n",
    "x_data, y_data = load_dance_dance_data_set(data_set_path, sampling_rate=sampling_rate, window_length=window_length)\n",
    "\n",
    "y_data = one_hot_encode_labels(y_data)\n",
    "\n",
    "x_tensor = torch.from_numpy(x_data)\n",
    "x_tensor = x_tensor.double()\n",
    "y_tensor = torch.from_numpy(y_data)\n",
    "y_tensor = y_tensor.double()\n",
    "input_size = x_data.shape[1]\n",
    "output_size = y_data.shape[1]\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim) \n",
    "\n",
    "#3 linear layer with ReLu activation for hidden and softmax activation for output\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, 256)\n",
    "        self.hidden2 = nn.Linear(256, 256)\n",
    "        self.hidden3 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, output_size)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.hidden1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.hidden2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.hidden3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(F.relu(x))\n",
    "        x = self.hidden3(F.relu(x))\n",
    "        x = self.output(F.relu(x))\n",
    "        return F.softmax(x, dim=0)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def prepare_data_loaders():\n",
    "\n",
    "    inputs, targets = load_dance_dance_data_set(data_set_path, sampling_rate=sampling_rate, window_length=window_length)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=8,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    return data_loader\n",
    "    \n",
    "def evaluate(model, criterion, data, target, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(data.shape[0]):\n",
    "            output = model(data[i])\n",
    "            #loss = criterion(output, target[i])\n",
    "            cnt += 1\n",
    "            #acc1, acc5 = accuracy(output, target[i], topk=(1, 5))\n",
    "            #print('.', end = '')\n",
    "            #top1.update(acc1[0], data.size(0))\n",
    "            #top5.update(acc5[0], data.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "        \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            maxk = max(topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res\n",
    "\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = Model()\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model_file = 'model_pytorch.pt'\n",
    "model_file_keras = 'trained_models/neural_network_model'\n",
    "\n",
    "model_keras = load_model_keras(model_file_keras)\n",
    "weights=model_keras.get_weights()\n",
    "\n",
    "model.hidden1.weight.data=torch.from_numpy(np.transpose(weights[0])).double()\n",
    "model.hidden1.bias.data=torch.from_numpy(weights[1]).double()\n",
    "model.hidden2.weight.data=torch.from_numpy(np.transpose(weights[2])).double()\n",
    "model.hidden2.bias.data=torch.from_numpy(weights[3]).double()\n",
    "model.hidden3.weight.data=torch.from_numpy(np.transpose(weights[4])).double()\n",
    "model.hidden3.bias.data=torch.from_numpy(weights[5]).double()\n",
    "model.output.weight.data=torch.from_numpy(np.transpose(weights[6])).double()\n",
    "model.output.bias.data=torch.from_numpy(weights[7]).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = torch.qnnpack #'fbgemm'#'qnnpack' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine (operator () at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qlinear_prepack.cpp:202)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7f5cdd0ea0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Convert to quantized model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Post Training Quantization: Convert done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\quantization\\quantize.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSWAPPABLE_MODULES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mreassign\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswap_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreassign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\quantization\\quantize.py\u001b[0m in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'qconfig'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDeQuantStub\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mnew_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\quantized\\modules\\linear.py\u001b[0m in \u001b[0;36mfrom_float\u001b[1;34m(cls, mod)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mweight_post_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Weight observer must have dtype torch.qint8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mqweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_quantize_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_post_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mqlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mqlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mqlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\quantized\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias_)\u001b[0m\n\u001b[0;32m    100\u001b[0m             [out_features, in_features], scale=1, zero_point=0, dtype=torch.qint8)\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packed_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearPackedParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packed_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\quantized\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearPackedParams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mwq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_empty_affine_quantized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\quantized\\modules\\linear.py\u001b[0m in \u001b[0;36mset_weight_bias\u001b[1;34m(self, weight, bias)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_weight_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# type: (torch.Tensor, Optional[torch.Tensor]) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packed_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_prepack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine (operator () at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qlinear_prepack.cpp:202)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "#y_data.astype(np.float)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_calibration_batches = 2000\n",
    "# Specify quantization configuration\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.default_qconfig\n",
    "print(model.qconfig)\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibration\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "evaluate(model, criterion, x_tensor, y_tensor, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model)\n",
    "\n",
    "#test if quantized success\n",
    "y_quantized = model(x_tensor[0])\n",
    "print(y_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
